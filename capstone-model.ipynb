{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Deep Convolutional Generative Adversarial Network (cDCGAN)\n",
    "\n",
    "### CIFAR10 Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gen_images(cifar_classes, cdcgan_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Progression for \"Dog\" Class\n",
    "\n",
    "I decided to only show the progression for a single class since there is low inter-class diversity, so the progression does not differ significantly between classes anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_progression('capstone-model-cifar-output/fake_images/', 'dog', fig_w = 20, fig_h = 40, nrows = 10, ncols = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novel Dataset Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gen_images(dataset.classes, cdcgan_novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Progression for \"Cyberpunk\" Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_progression('capstone-model-novel-output/fake_images/', 'cyberpunk', fig_w = 20, fig_h = 40, nrows = 10, ncols = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gridspec\n",
    "from torchsummary import summary\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = 'images'\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this size using a transformer.\n",
    "image_size_cifar = 32\n",
    "image_size_art = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(dataloader, classes, image_number = 8, model = None):\n",
    "    \n",
    "    '''\n",
    "    Function to plot a sample of images from the dataloader, alongside their class labels.\n",
    "    If a model is assigned to the model parameter, the predicted labels will be printed as well.\n",
    "    \n",
    "    Input:\n",
    "        dataloader (DATALOADER)\n",
    "            Dataloader of dataset.\n",
    "            \n",
    "        classes (ARR)\n",
    "            Array type object containing the class labels (strings) in the order that \n",
    "            corresponds with the numerical key in the dataloader.\n",
    "        \n",
    "        image_number (INT)\n",
    "            Number of images to plot from the dataloader. image_number should not exceed batch size.\n",
    "            Since images are plotted in a row, any number > 10 could cause display issues.\n",
    "            Default: 8.\n",
    "        \n",
    "        model (PYTORCH MODEL)\n",
    "            Optional parameter. If a model is provided, the predicted labels from the \n",
    "            model for each of the images will be printed as well. \n",
    "            Default: None.\n",
    "    '''\n",
    "    \n",
    "    # get images and true labels\n",
    "    images, labels = next(iter(dataloader))\n",
    "\n",
    "    # plot images\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(vutils.make_grid(images.to(device)[:image_number], padding=1, normalize=True).cpu(),(1,2,0)))\n",
    "    \n",
    "    # print true labels\n",
    "    print('True labels: ', '     '.join('%5s' % classes[labels[j]] for j in range(image_number)))\n",
    "    \n",
    "    if model:\n",
    "        # predict image classes using custom net\n",
    "        outputs = model(images)\n",
    "        # the outputs are energies for the 10 classes. \n",
    "        # the higher the energy for a class, the more the network thinks that the image is of the particular class.\n",
    "        # So, we get the index of the highest energy:\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # print predicted labels\n",
    "        print('Predicted:  ', '   '.join('%5s' % classes[predicted[j]] for j in range(image_number)))\n",
    "\n",
    "\n",
    "def get_target_index(dataset):\n",
    "    '''\n",
    "    Given a dataset, this function returns a dictionary of classes, where the value of each class \n",
    "    is a dictionary containing the class indices and the number of datapoints in the class.\n",
    "    \n",
    "    Input:\n",
    "        dataset (IMAGEFOLDER)\n",
    "            Dataset should be ImageFolder class.\n",
    "        \n",
    "    Output:\n",
    "        idx_dct (DCT)\n",
    "            Nested dictionary with the class name as key, and a dictionary containing the\n",
    "            'indices' and 'length' of the class as values.\n",
    "            Example format:\n",
    "            idx_dct = { 'class_A':{\n",
    "                        'indices': [1,2,3,4,5],\n",
    "                        'length': 5\n",
    "                        },\n",
    "                        'class_B':{\n",
    "                        'indices': [6,7,8],\n",
    "                        'length': 3\n",
    "                        },\n",
    "                        'class_C':{\n",
    "                        'indices': [100,101,102,103],\n",
    "                        'length': 4\n",
    "                        }}\n",
    "    '''\n",
    "    targets = torch.tensor([t[1] for t in dataset.samples])\n",
    "    idx_dct = {}\n",
    "    \n",
    "    for k,v in dataset.class_to_idx.items():\n",
    "        idx_dct[k] = {'indices': (targets == v).nonzero().reshape(-1)}\n",
    "        idx_dct[k]['length'] = len(idx_dct[k]['indices'])\n",
    "        \n",
    "    return idx_dct\n",
    "\n",
    "\n",
    "def plot_batch(dataloader):    \n",
    "    '''\n",
    "    Plot images from a dataloader\n",
    "    '''\n",
    "    real_batch = next(iter(dataloader))\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform images to tensor and normalize\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# create dataloaders\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "cifar_classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# plot sample of images\n",
    "plot_images(trainloader, cifar_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novel Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                           transforms.Resize(image_size_art),\n",
    "                           transforms.CenterCrop(image_size_art),\n",
    "                           transforms.RandomHorizontalFlip(p=0.5),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "target_idx_dct = get_target_index(dataset)\n",
    "for k,v in target_idx_dct.items():\n",
    "    print(f\"Class {k} has {v['length']} entries.\")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "plot_images(dataloader, dataset.classes, image_number = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cDCGAN Model\n",
    "\n",
    "### Weights Initialization\n",
    "\n",
    "From Inkawhich (2017):\n",
    "\n",
    "From the DCGAN paper, the authors specify that all model weights shall be randomly initialized from a Normal distribution with mean=0, stdev=0.02. The weights_init function takes an initialized model as input and reinitializes all convolutional, convolutional-transpose, and batch normalization layers to meet this criteria. This function is applied to the models immediately after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    '''\n",
    "    Custom weights initialization called on netG and netD\n",
    "    '''\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_64(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, ngpu, ndf = 64):\n",
    "        '''\n",
    "        Discriminator for images with dimensions: 64 x 64 x 3.\n",
    "        \n",
    "        Inputs\n",
    "            n_classes (INT)\n",
    "                Number of classes in dataset.\n",
    "            \n",
    "            ngpu (INT)\n",
    "                Number of GPUs to be used in training process.\n",
    "                \n",
    "            ndf (INT)\n",
    "                Size of feature maps in discriminator.\n",
    "                Default: 64.\n",
    "        '''\n",
    "        \n",
    "        super(Discriminator_64, self).__init__()\n",
    "        \n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.label_embedding = nn.Embedding(n_classes, n_classes)\n",
    "        \n",
    "        self.convolution_layers = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "        )\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(in_features = 1 + n_classes, # flattened output from last conv + embedding\n",
    "                      out_features = 512), # arbitrary + based on external references\n",
    "            \n",
    "            nn.LeakyReLU(0.2, inplace=True) ,\n",
    "        \n",
    "            nn.Linear(in_features = 512, # output from last linear layer\n",
    "                      out_features = 1), # true or false image\n",
    "            \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        \n",
    "        x = self.convolution_layers(input) # run input through convolutional layers\n",
    "        # print(x.shape) # output shape: (128,1,1,1)\n",
    "        x = x.view(x.size(0), -1) # flatten output from main\n",
    "        # print(x.shape) # output shape: (128,1)\n",
    "        y = self.label_embedding(labels) # create label layer\n",
    "        # print(y.shape) # output shape: (128,3)\n",
    "        x = torch.cat((x, y), -1) # concatenate flattened output to label layer\n",
    "        # print(x.shape) # output shape: (128,4)\n",
    "        x = self.linear_layers(x) # run flattened + merged layer through linear layers\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_32(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, ngpu, ndf = 32):\n",
    "        '''\n",
    "        Discriminator for images with dimensions: 32 x 32 x 3.\n",
    "        \n",
    "        Inputs\n",
    "            n_classes (INT)\n",
    "                Number of classes in dataset.\n",
    "            \n",
    "            ngpu (INT)\n",
    "                Number of GPUs to be used in training process.\n",
    "                \n",
    "            ndf (INT)\n",
    "                Size of feature maps in discriminator.\n",
    "                Default: 32.\n",
    "        '''\n",
    "        \n",
    "        super(Discriminator_32, self).__init__()\n",
    "        \n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.label_embedding = nn.Embedding(n_classes, n_classes)\n",
    "        \n",
    "        self.convolution_layers = nn.Sequential(\n",
    "            # input is (nc) x 32 x 32\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 16 x 16\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*2) x 8 x 8\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 4 x 4\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
    "        )\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(in_features = 1 + n_classes, # flattened output from last conv + embedding\n",
    "                      out_features = 512), # arbitrary + based on external references\n",
    "            \n",
    "            nn.LeakyReLU(0.2, inplace=True) ,\n",
    "        \n",
    "            nn.Linear(in_features = 512, # output from last linear layer\n",
    "                      out_features = 1), # true or false image\n",
    "            \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        \n",
    "        x = self.convolution_layers(input) # run input through convolutional layers\n",
    "        # print(x.shape) # output shape: (128,1,1,1)\n",
    "        x = x.view(x.size(0), -1) # flatten output from main\n",
    "        # print(x.shape) # output shape: (128,1)\n",
    "        y = self.label_embedding(labels) # create label layer\n",
    "        # print(y.shape) # output shape: (128,3)\n",
    "        x = torch.cat((x, y), -1) # concatenate flattened output to label layer\n",
    "        # print(x.shape) # output shape: (128,4)\n",
    "        x = self.linear_layers(x) # run flattened + merged layer through linear layers\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator_64(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, ngpu, ngf = 64):\n",
    "        '''\n",
    "        Generator for images with dimensions: 64 x 64 x 3.\n",
    "        \n",
    "        Inputs\n",
    "            n_classes (INT)\n",
    "                Number of classes in dataset.\n",
    "            \n",
    "            ngpu (INT)\n",
    "                Number of GPUs to be used in training process.\n",
    "                \n",
    "            ngf (INT)\n",
    "                Size of feature maps in generator.\n",
    "                Default: 64.\n",
    "        '''\n",
    "        \n",
    "        super(Generator_64, self).__init__()\n",
    "        \n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.label_emb = nn.Embedding(n_classes, n_classes)\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            # input is Z + n_classes, going into a convolution\n",
    "            nn.ConvTranspose2d(nz + n_classes, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "    \n",
    "        # Concatenate label embedding and noise to produce input\n",
    "        flat_embed_input = torch.cat((self.label_emb(labels), input), -1)\n",
    "\n",
    "        # reshape flattened layer to torch.Size([128, nz + n_classes, 1, 1])\n",
    "        reshaped_input = flat_embed_input.view((-1,nz + self.n_classes,1,1)) \n",
    "        \n",
    "        gen_img = self.main(reshaped_input)\n",
    "        \n",
    "        return gen_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator_32(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, ngpu, ngf = 32):\n",
    "        '''\n",
    "        Generator for images with dimensions: 32 x 32 x 3.\n",
    "        \n",
    "        Inputs\n",
    "            n_classes (INT)\n",
    "                Number of classes in dataset.\n",
    "            \n",
    "            ngpu (INT)\n",
    "                Number of GPUs to be used in training process.\n",
    "                \n",
    "            ngf (INT)\n",
    "                Size of feature maps in generator.\n",
    "                Default: 32.\n",
    "        '''\n",
    "        \n",
    "        super(Generator_32, self).__init__()\n",
    "        \n",
    "        self.ngpu = ngpu\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.label_emb = nn.Embedding(n_classes, n_classes)\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            # input is Z + n_classes, going into a convolution\n",
    "            nn.ConvTranspose2d(nz + n_classes, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 32 x 32\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "    \n",
    "        # Concatenate label embedding and noise to produce input\n",
    "        flat_embed_input = torch.cat((self.label_emb(labels), input), -1)\n",
    "\n",
    "        # reshape flattened layer to torch.Size([128, nz+n_classes, 1, 1])\n",
    "        reshaped_input = flat_embed_input.view((-1,nz + self.n_classes,1,1)) \n",
    "        \n",
    "        gen_img = self.main(reshaped_input)\n",
    "        \n",
    "        return gen_img\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional DCGAN Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cDCGAN(object):\n",
    "    \n",
    "    '''\n",
    "    Conditional DCGAN class.\n",
    "    '''\n",
    "    \n",
    "    def _checkDirectory(self, dirName):\n",
    "        \n",
    "        if not os.path.exists(dirName):\n",
    "            print(f\"{dirName} directory does not exist. Making {dirName}\")\n",
    "            os.makedirs(dirName)\n",
    "            \n",
    "        else: print(f\"{dirName} directory exists.\")\n",
    "    \n",
    "    \n",
    "    def __init__(self, dataloader, classes, save_dir, num_epochs,\n",
    "                 criterion, netD, netG, optimizerD, optimizerG, device):\n",
    "        \n",
    "        # data parameters\n",
    "        self.dataloader = dataloader\n",
    "        self.classes = classes # class labels\n",
    "        self.n_classes = len(classes) # number of classes\n",
    "        \n",
    "        # save file locations\n",
    "        self._checkDirectory(save_dir) # check whether save dir exists\n",
    "        self.checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
    "        self._checkDirectory(self.checkpoint_dir) # create checkpoints dir\n",
    "        self.fake_image_dir = os.path.join(save_dir, 'fake_images')\n",
    "        self._checkDirectory(self.fake_image_dir) # create fake images dir\n",
    "        \n",
    "        # model parameters\n",
    "        self.num_epochs = num_epochs # number of epochs to train for\n",
    "        self.start_epoch = 1 # the starting epoch\n",
    "        self.criterion = criterion # loss function\n",
    "        self.real_label = 1 # Establish convention for real and fake labels during training\n",
    "        self.fake_label = 0\n",
    "\n",
    "        # networks init\n",
    "        self.netD = netD\n",
    "        self.netG = netG\n",
    "        self.optimizerD = optimizerD\n",
    "        self.optimizerG = optimizerG\n",
    "        \n",
    "        # device\n",
    "        self.device = device # specify device being used\n",
    "        \n",
    "        # Create fixed noise to visualize the progression of the generator\n",
    "        self.fixed_noise = torch.randn(64, nz, device=self.device) # torch.Size([64, 100])\n",
    "            \n",
    "        \n",
    "    def generate_fake_images(self, class_index_tensor, noise, image_name = 'random', save = True):\n",
    "        \n",
    "        '''\n",
    "        Generate a batch of fake images using current generator weights.\n",
    "        \n",
    "        Inputs\n",
    "        \n",
    "            class_index_tensor (LongTensor)\n",
    "                The class index to create fake images for. The number of fake images generated is equal\n",
    "                to the length of the tensor. So a tensor filled with 10 \"1\"s will generate 10 images for\n",
    "                the class that corresponds to \"1\".\n",
    "                \n",
    "            noise (Tensor)\n",
    "                Random noise that will be put through the generator weights to produce an image.\n",
    "        \n",
    "            image_name (STR)\n",
    "                Image name for the saved file.\n",
    "                If running this function in model training, image_name should contain a changing variable,\n",
    "                otherwise the files will just keep overwriting each other with the same name.\n",
    "                Default: 'random' (in case save = True but no image_name provided)\n",
    "            \n",
    "            save (BOOL)\n",
    "                If save is TRUE, the image file will be saved in the specified \"self.fake_image_dir\".\n",
    "                Otherwise, just return the image data for plotting.\n",
    "                Default: TRUE\n",
    "            \n",
    "        ''' \n",
    "        with torch.no_grad():\n",
    "            # create fake images for a the labels in class_index_tensor\n",
    "            fake = self.netG(noise, class_index_tensor).detach().cpu()\n",
    "        \n",
    "        if save: # save images in the fake_image_dir\n",
    "            save_image(fake.data, f'{self.fake_image_dir}/{image_name}.png',\n",
    "                       nrow=8, padding=2, normalize=True)\n",
    "        \n",
    "        return fake.data\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        '''\n",
    "        Training loop\n",
    "        '''\n",
    "        if self.num_epochs == 0:\n",
    "            print(f\"No epochs set for training. Exiting training loop.\")\n",
    "            return\n",
    "            \n",
    "        # Lists to keep track of progress\n",
    "        self.G_losses = [] # generator loss\n",
    "        self.D_losses = [] # discriminator loss\n",
    "        iters = 0\n",
    "\n",
    "        print(\"Starting Training Loop...\")\n",
    "        # For each epoch\n",
    "        for epoch in range(self.start_epoch, self.start_epoch + self.num_epochs):\n",
    "            # For each batch in the dataloader\n",
    "            for i, (imgs, class_labels) in enumerate(self.dataloader):\n",
    "\n",
    "                ############################\n",
    "                # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "                ###########################\n",
    "\n",
    "                ## Train with all-real batch\n",
    "                ###########################\n",
    "                self.netD.zero_grad()\n",
    "\n",
    "                # Format batch\n",
    "                real_imgs = imgs.to(self.device)\n",
    "                b_size = real_imgs.size(0)\n",
    "\n",
    "                # Set ground truth labels as REAL\n",
    "                validity_label = torch.full((b_size,), self.real_label, device=device)\n",
    "\n",
    "                # Forward pass real batch through D\n",
    "                output = self.netD(real_imgs, class_labels).view(-1)\n",
    "\n",
    "                # Calculate loss on all-real batch\n",
    "                errD_real = self.criterion(output, validity_label)\n",
    "\n",
    "                # Calculate gradients for D in backward pass\n",
    "                errD_real.backward()\n",
    "                D_x = output.mean().item()\n",
    "\n",
    "\n",
    "                ## Train with all-fake batch\n",
    "                ###########################\n",
    "                # Generate batch of latent vectors\n",
    "                noise = torch.randn(b_size, nz, device=device) # torch.Size([128, 10])\n",
    "\n",
    "                # Generate batch of fake labels\n",
    "                gen_labels = torch.randint(self.n_classes, (b_size,)).type(torch.LongTensor) # torch.Size([128, 3])\n",
    "\n",
    "                # Generate fake image batch with G\n",
    "                fake = self.netG(noise, gen_labels)\n",
    "\n",
    "                # Update ground truth labels to FAKE\n",
    "                validity_label.fill_(self.fake_label)\n",
    "\n",
    "                # Classify all fake batch with D\n",
    "                output = self.netD(fake.detach(), gen_labels).view(-1)\n",
    "\n",
    "                # Calculate D's loss on the all-fake batch\n",
    "                errD_fake = self.criterion(output, validity_label)\n",
    "\n",
    "                # Calculate the gradients for this batch\n",
    "                errD_fake.backward()\n",
    "                D_G_z1 = output.mean().item()\n",
    "\n",
    "                # Add the gradients from the all-real and all-fake batches\n",
    "                errD = errD_real + errD_fake\n",
    "\n",
    "                # Update D\n",
    "                self.optimizerD.step()\n",
    "\n",
    "                ############################\n",
    "                # (2) Update G network: maximize log(D(G(z)))\n",
    "                ###########################\n",
    "\n",
    "                self.netG.zero_grad()\n",
    "\n",
    "                validity_label.fill_(self.real_label)  # fake labels are real for generator cost\n",
    "                \n",
    "                # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "                output = self.netD(fake, gen_labels).view(-1)\n",
    "                \n",
    "                # Calculate G's loss based on this output\n",
    "                errG = self.criterion(output, validity_label)\n",
    "                \n",
    "                # Calculate gradients for G\n",
    "                errG.backward()\n",
    "                D_G_z2 = output.mean().item()\n",
    "                \n",
    "                # Update G\n",
    "                self.optimizerG.step()\n",
    "\n",
    "                # Output training stats\n",
    "                if i % 50 == 0:\n",
    "                    print(f'[{epoch}/{self.start_epoch + self.num_epochs - 1}][{i}/{len(self.dataloader)}]\\tLoss_D: {round(errD.item(),2)}\\tLoss_G: {round(errG.item(),2)}\\tD(x): {round(D_x,2)}\\tD(G(z)): {round(D_G_z1/D_G_z2,2)}')\n",
    "\n",
    "                # Save Losses for plotting later\n",
    "                self.G_losses.append(errG.item())\n",
    "                self.D_losses.append(errD.item())\n",
    "\n",
    "                # Check how the generator is doing by saving G's output on fixed_noise\n",
    "                # every 500 iterations, or on the last batch of the last epoch\n",
    "                if (iters % 500 == 0) or ((epoch == self.num_epochs-1) and (i == len(self.dataloader)-1)):\n",
    "                    \n",
    "                    print(\"Saving a batch of fake images.\")\n",
    "                    \n",
    "                    class_index = torch.arange(self.n_classes) # get class indices\n",
    "                    for i in class_index:\n",
    "                        class_index_tensor = torch.LongTensor(64).fill_(i) # repeat the same class index 10 times\n",
    "                        self.generate_fake_images(class_index_tensor, self.fixed_noise,\n",
    "                                                  image_name = f'{self.classes[i]}_e{epoch}', save = True)\n",
    "\n",
    "                iters += 1\n",
    "\n",
    "            # automatically save model for first epoch (testing) and every 5 epochs\n",
    "            if epoch == 1 or epoch % 5 == 0: self.save(epoch)\n",
    "\n",
    "        print(f\"Finished Training for {epoch} epochs.\")\n",
    "        self.save(epoch)\n",
    "        \n",
    "        \n",
    "    def save(self, epoch):\n",
    "        \n",
    "        # save the model checkpoint\n",
    "        filepath = f'{self.checkpoint_dir}/checkpoint_e{epoch}.pth.tar'\n",
    "        print(f\"=> Saving checkpoint: {filepath}\")\n",
    "\n",
    "        state = {\n",
    "            'D_losses': self.D_losses,\n",
    "            'G_losses': self.G_losses,\n",
    "            'epoch': epoch,\n",
    "            'netD_state_dict': self.netD.state_dict(),\n",
    "            'optimizerD': self.optimizerD.state_dict(),\n",
    "            'netG_state_dict': self.netG.state_dict(),\n",
    "            'optimizerG': self.optimizerG.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(state, filepath) \n",
    "\n",
    "        \n",
    "    def load(self, loadpath):\n",
    "        '''\n",
    "        When loading model checkpoint, just load the epoch and state dicts to continue training.\n",
    "        The D-loss and G-loss can be stored within their respective checkpoints\n",
    "        and referred to later when needed.\n",
    "        '''\n",
    "        if os.path.isfile(loadpath):\n",
    "            print(f\"=> loading checkpoint: {loadpath}\")\n",
    "            checkpoint = torch.load(loadpath)\n",
    "\n",
    "            self.start_epoch = checkpoint['epoch'] + 1\n",
    "            self.netD.load_state_dict(checkpoint['netD_state_dict'])\n",
    "            self.netG.load_state_dict(checkpoint['netG_state_dict'])\n",
    "            self.optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "            self.optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "\n",
    "            print(f\"=> loaded checkpoint: {loadpath}\")\n",
    "            print(f\"Last epoch was {checkpoint['epoch']}\")\n",
    "\n",
    "        else: \n",
    "            print(f\"=> No checkpoint found at: {loadpath}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator_32(len(cifar_classes), ngpu).to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)\n",
    "\n",
    "# Create the generator\n",
    "netG = Generator_32(len(cifar_classes), ngpu).to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'dataloader': trainloader, # cifar dataloader\n",
    "    'classes': cifar_classes, # cifar classes\n",
    "    'save_dir':'capstone-model-cifar-output',\n",
    "    'num_epochs': 25,\n",
    "    'criterion': nn.BCELoss(), # Initialize BCELoss function\n",
    "    'netD': netD,\n",
    "    'netG': netG,\n",
    "    'optimizerD': optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)),\n",
    "    'optimizerG': optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999)),\n",
    "    'device': device\n",
    "}\n",
    "    \n",
    "cdcgan_cifar = cDCGAN(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcgan_cifar.load('capstone-model-cifar-output/checkpoints/checkpoint_e101.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcgan_cifar.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novel Dataset Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator_64(len(dataset.classes), ngpu).to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)\n",
    "\n",
    "# Create the generator\n",
    "netG = Generator_64(len(dataset.classes), ngpu).to(device)\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'dataloader': dataloader, # novel data dataloader\n",
    "    'classes': dataset.classes, # novel dataset classes\n",
    "    'save_dir':'capstone-model-novel-output',\n",
    "    'num_epochs': 25,\n",
    "    'criterion': nn.BCELoss(), # Initialize BCELoss function\n",
    "    'netD': netD,\n",
    "    'netG': netG,\n",
    "    'optimizerD': optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999)),\n",
    "    'optimizerG': optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999)),\n",
    "    'device': device\n",
    "}\n",
    "    \n",
    "cdcgan_novel = cDCGAN(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcgan_novel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcgan_novel.load('capstone-model-novel-output/checkpoints/checkpoint_e175.pth.tar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Visualizing Results\n",
    "\n",
    "The following functions were used to visualize the generated images. The output is seen in the results section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gen_images(classes, model, no_of_images = 10):\n",
    "    '''\n",
    "    Plots the generated images for each class.\n",
    "    \n",
    "    Inputs\n",
    "        classes (ARR)\n",
    "            List of classes to be used as labels for generating fake images.\n",
    "        \n",
    "        model (MODEL)\n",
    "            Pytorch model to be used for generating fake images.\n",
    "        \n",
    "        no_of_images (INT)\n",
    "            Number of fake images to generated for each class.\n",
    "            Default: 10.\n",
    "    '''\n",
    "    noise = torch.randn(no_of_images, nz, device=device)\n",
    "\n",
    "    for c in range(len(classes)):\n",
    "        images = model.generate_fake_images(torch.LongTensor(no_of_images).fill_(c), noise = noise, save = False)\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Fake {classes[c]}', size=20)\n",
    "        plt.imshow(np.transpose(vutils.make_grid(images, nrow = no_of_images, normalize = True), (1,2,0)))\n",
    "\n",
    "\n",
    "def plot_progression(my_dir, class_name, fig_w, fig_h, nrows, ncols, info = False):\n",
    "    '''\n",
    "    Plot the cDCGAN's image generation progression throughout the training process.\n",
    "    \n",
    "    Inputs\n",
    "        my_dir (STR)\n",
    "            Directory of generator progression images. \n",
    "            Images should be labelled with digit numbers for sorting.\n",
    "            \n",
    "        class_name (STR)\n",
    "            Specific class label to be plotted.\n",
    "            \n",
    "        fig_w (INT)\n",
    "            Figure width.\n",
    "        \n",
    "        fig_h (INT)\n",
    "            Figure height.\n",
    "  \n",
    "        nrows (INT)\n",
    "            Number of rows in plot.\n",
    "            \n",
    "        ncols (INT)\n",
    "            Number of columns in plot.\n",
    "            \n",
    "        info (BOOL)\n",
    "            If true, displays the number of images to be plotted and the list of image names.\n",
    "            Default: False.\n",
    "    '''\n",
    "    dir_list = os.listdir(my_dir) # get list of images in directory\n",
    "    img_list = [i for i in dir_list if class_name in i] # get list of images in a specific class\n",
    "    img_list.sort(key = lambda f: int(''.join(filter(str.isdigit, f)))) # sort in ascending order\n",
    "    \n",
    "    if info:\n",
    "        print(f\"There are {len(img_list)} images to be plotted. Adjust params accordingly.\")\n",
    "        print(f\"Images: {img_list}\")\n",
    "    \n",
    "    plt.figure(figsize=(fig_w, fig_h))\n",
    "    gs1 = gridspec.GridSpec(nrows, ncols)\n",
    "    gs1.update(wspace=0, hspace=0.025) # set the spacing between axes. \n",
    "\n",
    "    # view generator progression\n",
    "    for i in range(len(img_list)):\n",
    "        ax = plt.subplot(gs1[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_aspect('equal')\n",
    "        img = mpimg.imread(f\"{my_dir}/{img_list[i]}\")\n",
    "        plt.imshow(img)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
